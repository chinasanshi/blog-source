#+TITLE:       Deep Learning
#+AUTHOR:      Kyle Three Stones
#+DATE:        <2018-07-12 Thu 07:19>
#+EMAIL:       kyleemail@163.com
#+OPTIONS:     H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t f:t tex:t
#+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler
#+STARTUP: latexpreview
#+TAGS:        深度学习, 机器学习
#+CATEGORIES:  深度学习

1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任
意m 维到 n 维的映射）。

** 神经网络和深度学习

*** 神经网络概论
ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值。

每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会认为决定每个神经元
的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神
经网络在监督学习中效果很好很强大。

+ 结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等
+ 非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字
人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣
钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也
可以处理非结构化的数据。

神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以
及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的
算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法]

scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神
经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好
的 SVM 算法结果要由于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在在达到一定的性能之后效果变
无法继续提升，而神经网络此时的效果将明显领先与传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算
法都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速
度，因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，
从而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高。速度的提升使得我
们可以训练大型的网络或者在一定的时间内完成网络的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断
循环，迭代的更快使得验证自己的想法更加快速得到验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。


*** 神经网络基础

一张彩色图像像素点由 RGB 三个通道组成，作为神经网络的输入时，将三个矩阵都转换成向量并拼接起来组成一个列向量 \(x^{(i)}
\in \mathbb{R}^{n_x}\)，列向量中先是红色通道的所有像素点，然后是绿色通道的所有像素点，最后是蓝色通道的所有像素点。m 个训
练样本 \(\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(i)},y^{(i)}) \}\) ；同时使用 \(X \in \mathbb{R}^{n_x
\times m} \) 表示所有的训练样本\[X= \left[ \begin{array}{cccc} | & | & & | \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\ |
& | & & | \end{array} \right] \] 相比于让每个样本按行向量堆叠，在神经网络中构建过程会简单很多。\(y^{(i)} \in \{0,1\}\)
同时将所有的标签组成一个行向量 \(Y \in \mathbb{R}^{1 \times m}\) \[ Y = [ y^{(1)}, y^{(2)}, \cdots, y^{(m)} ]\] 。在
Python 中使用 (n,m) = X.shape 和 (1,m) = Y.shape 得到向量的维数（使用的是 numpy 库中的array）。并且在神经网络中，使用权
重 w 和基 b 来表示参数，而不使用 \(\theta\) 来整体表示参数。且在程序中使用变量 dw 和 db 来表示 cost function 对 w 和 b
的导数。

loss(error) function 定义的是单个样本的误差；cost function 衡量的是在所有训练样本上的性能，定义为所有样本的 loss
function 之和的平均。算法通过 cost function 来求解参数 w 和 b 。

梯度下降法几乎对任何初始化方法都有效，如初始化为 0 或者使用随机值进行初始化。

*** 一层神经元网络

*** 多层神经元网络

** 提升深度神经元网络：超参调节、正则化、最优化


** 让机器学习工程变得结构化


** 卷积神经网络


** 自然语言处理


