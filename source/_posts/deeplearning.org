#+TITLE:          Deep Learning
#+AUTHOR:         Kyle Three Stones
#+DATE:           <2018-07-12 Thu 07:19>
#+EMAIL:          kyleemail@163.com
#+OPTIONS:        H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t f:t tex:t
#+HTML_MATHJAX:   align: left indent: 5em tagside: left font: Neo-Euler
#+STARTUP:        latexpreview
#+TAGS:           深度学习, 机器学习
#+CATEGORIES:     深度学习

1989 年 Robert Hecht-Nielsen 证明了万能逼近定理：对于任何闭区间的一个连续函数都可以用一个隐含层的 BP 网络来逼近（完成任
意m 维到 n 维的映射）。

** 神经网络和深度学习

*** 神经网络概论
ReLu: rectified linear unit ，修正线性单元；修正指的是取不小于 0 的值。

每个神经元类似一个乐高积木(Lego brick) ，将许多神经元堆叠在一起就形成了一个较大的神经网络。而且并不会认为决定每个神经元
的作用，而是由神经网络自己决定每个神经元的作用。如果给神经网络足够多的训练数据，其非常擅长计算从输入到输出的精确映射。神
经网络在监督学习中效果很好很强大。

+ 结构化数据(structured data)：每个特征都有清晰、明确有意义的定义；比如房屋的面积，人的身高等
+ 非结构化数据(unstructured data)：特征无法精确定义；比如图像的像素点，音频，文字
人类很擅长处理结构化的数据，但机器很不擅长。而归功于深度学习，使得机器在非结构化数据的处理有了明显的提高；但是现在比较挣
钱的仍然是让机器处理结构化数据，如广告投放、理解处理公司的海量数据并进行预测等。吴恩达希望设计的网络可以处理结构化数据也
可以处理非结构化的数据。

神经网络有不同的种类，有用于处理图像的 CNN(Convolution Neural Network)、处理一维序列的 RNN(Recurrent Neural Network)、以
及自动驾驶中用于处理雷达数据的混合神经网络(Hybrid Neural Network)[对于复杂的问题，需要自行构建网络的架构；和机器学习中的
算法一样，针对具体的问题，需要去做具体的优化，而不是一成不变的使用基本的算法]

scale 使得神经网络在最近流行起来，这里的 scale 并不单单指神经网络的规模，还包括数据的规模。当训练样本不是很大的时候，神
经网络与传统的机器学习算法之间的优劣并不明显，此时主要取决有人为设计算法的技巧和能力以及算法处理的细节，可能一个设计良好
的 SVM 算法结果要由于一个神经网络的效果；但是随着样本量不断变大，传统的机器学习算法的性能会在在达到一定的性能之后效果变
无法继续提升，而神经网络此时的效果将明显领先与传统的算法[需要很大的样本，且网络的规模越大，性能越好]。数据、计算能力、算
法都促使了深度学习的发展；算法的主要改进都在加快算法的速度，比如使用 ReLU 函数替代 sigmoid 函数就大大加快了算法的训练速
度，因为 sigmoid 函数在自变量趋向于正负无穷大的时候，导数趋向于 0，而使用梯度下降法，梯度的减小将使得参数的变化变得缓慢，
从而学习将变得缓慢；而 ReLU 函数右侧的斜率始终为 1，由于斜率不会逐渐趋向于 0，使得算法训练速度大大提高。速度的提升使得我
们可以训练大型的网络或者在一定的时间内完成网络的训练。而且训练神经网络的过程一般是 idea - code - experiment - idea 不断
循环，迭代的更快使得验证自己的想法更加快速得到验证，将有机会取验证更多的想法，从而更有可能找到合适的结果。


*** 神经网络基础

一张彩色图像像素点由 RGB 三个通道组成，作为神经网络的输入时，将三个矩阵都转换成向量并拼接起来组成一个列向量 \(x^{(i)}
\in \mathbb{R}^{n_x}\)，列向量中先是红色通道的所有像素点，然后是绿色通道的所有像素点，最后是蓝色通道的所有像素点。m 个训
练样本 \(\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \cdots, (x^{(i)},y^{(i)}) \}\) ；同时使用 \(X \in \mathbb{R}^{n_x
\times m} \) 表示所有的训练样本\[X= \left[ \begin{array}{cccc} | & | & & | \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\ |
& | & & | \end{array} \right] \] 相比于让每个样本按行向量堆叠，在神经网络中构建过程会简单很多。\(y^{(i)} \in \{0,1\}\)
同时将所有的标签组成一个行向量 \(Y \in \mathbb{R}^{1 \times m}\) \[ Y = [ y^{(1)}, y^{(2)}, \cdots, y^{(m)} ]\] 。在
Python 中使用 (n,m) = X.shape 和 (1,m) = Y.shape 得到向量的维数（使用的是 numpy 库中的array）。并且在神经网络中，使用权
重 w 和基 b 来表示参数，而不使用 \(\theta\) 来整体表示参数。且在程序中使用变量 dw 和 db 来表示 cost function 对 w 和 b
的导数。

loss(error) function 定义的是单个样本的误差；cost function 衡量的是在所有训练样本上的性能，定义为所有样本的 loss
function 之和的平均。算法通过 cost function 来求解参数 w 和 b 。

梯度下降法几乎对任何初始化方法都有效，如初始化为 0 或者使用随机值进行初始化。

导数，也就是斜率，定义为自变量在某一点产生一点变化，导致因变量变化值相对于自变量变化值的倍数。导数在不同的点可能会有不同
的值，由此组成了导函数。

计算图(computation graph) 用于精确的描述反向传播算法。图中每个节点表示一个变量（变量可以是标量、向量、矩阵、张量或者其他
类型的变量），操作(operation)指一个或多个变量的简单函数[深度学习书中定义一个操作仅返回单个输出变量]。

由于计算反向传播的过程中很多都在求解最终的损失函数(cost function)对中间某个变量的导数，如 \(\frac{d}{dw_1}J\) ，所以一般
会将该表达式简写成 \(dw_1\) 。

向量化可以大大提高运算速度。 CPU 和 GPU 都有并行化的指令，称为 SIMD(single instruction multiple data)[单指令流多数据流]。
CPU 也有并行化指令，只是没有 GPU 那么擅长。使用 Python 的 numpy 库函数能够充分利用并行化操作来提高计算速度，这些库函数都
进行了很好的运用了并行化指令。 *原则，如果有其他的方法，就不要使用 for 循环。*

将每个样本 \(x^{(i)}\) 看成一个列向量，然后按列把所有样本堆叠起来组成一个大的矩阵 X 。权重 \(W^T\) 视为行向量，可以让两
者直接相乘得到 \(W^T X\) ，而不再是逐个样本去计算。

#+BEGIN_SRC python
import numpy as np
A = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12] ])
print(A.shape)
cal = A.sum(axis=0) # 按列求和
row = A.sum(axis=1) # 按行求和
# numpy 中有广播机制，可以自动扩展向量（按行或者列复制 n 次）
percentage = 100 * A / (cal.reshape(1,4) # 最好使用 reshape 函数确保
# 矩阵的维数正确，该函数调用成本很低 O(1)

a = np.random.randn(5) # 
print(a.shape) # (5,) 是一个秩为 1 的数组，但即不是行向量，也不是列向量；永远不要使用，否则会产生一些很奇怪的 bug

a = np.random.randn(5,1) # 列向量
a.shape # (5,1)
a = np.random.randn(1,5) # 行向量
a.shape # (1,5)

assert(a.shape == (5,1)) # 多多验证
a = a.reshape((5,1))

db = np.sum(dz, axis = 1, keepdims = True) # keepdims 用于阻止 numpy 生成秩为 1 的数组
#+END_SRC

*** 两层神经元网络

\(z^{[i]}\) 用于表示网络的第 i 层。

输入层、隐层、输出层。约定俗成， *计算网络的层数的时候，不算输入层，输入层称为第 0 层。*
\(a^{[0]} = X \) 表示输入层 (a 是 activation)， \(a^{[1]}\) 表示第一个隐层
\[a^{[1]} = \left[ \begin{array}{c} a_1^{[1]} \\ a_2^{[1]} \\ \ldots \\ a_{m1}^{[1]} \end{array} \right] \]

\[ z_1^{[1]} = {(w_1^{[1]})}^T x + b_1^{[1]}, \ a_1 = sigmoid(z_1^{[1]}) \]
\[ z_2^{[1]} = {w_2^{[1]}}^T x + b_2^{[1]}, \ a_2 = sigmoid(z_2^{[1]}) \]
\[ z_3^{[1]} = {w_3^{[1]}}^T x + b_3^{[1]}, \ a_3 = sigmoid(z_3^{[1]}) \]
\[ z_4^{[1]} = {w_4^{[1]}}^T x + b_4^{[1]}, \ a_4 = sigmoid(z_4^{[1]}) \]

将网络中每一层的相同变量按行堆叠起来组成一个列向量，如 w, b, z, a ，便可以使用向量化计算来提高速度。

\[ W^{[i]} = \left[ \begin{array}{ccc} -- & {w_1^{[i]}}^T & -- \\ -- & {w_2^{[i]}}^T & -- \\ 
& \vdots & \\ -- & {w_l^{[i]}}^T & -- \end{array} \right] \]
\[ b^{[i]} = \left[ \begin{array}{c} b_1^{[i]} \\ b_2^{[i]} \\ \vdots \\ b_l^{[i]} \end{array} \right] \]
\[ z^{[i]} = \left[ \begin{array}{c} z_1^{[i]} \\ z_2^{[i]} \\ \vdots \\ z_l^{[i]} \end{array} \right] \]
\[ a^{[i]} = \left[ \begin{array}{c} a_1^{[i]} \\ a_2^{[i]} \\ \vdots \\ a_l^{[i]} \end{array} \right] \]

得到向量化公式

\[ z^{[i]} = W^{[i]} a^{[i-1]} + b^{[i]} \]
\[ a^{[i]} = np.sigmoid(z^{[i]}) \]

多个训练样本
\[ z^{[i](l)} = W^{[i]} a^{[i-1](l)} + b^{[i]} \]
\[ a^{[i](l)} = sigmoid(z^{[i](l)}) \]

不同训练样本的值按列堆叠

\[X= \left[ \begin{array}{cccc} | & | & & | \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)} \\ |
& | & & | \end{array} \right] \]

\[Z^{[i]} = \left[ \begin{array}{cccc} | & | & & | \\ z^{[i](1)} & z^{[i](2)} & \cdots & z^{[i](m)} \\ |
& | & & | \end{array} \right] \]

\[A^{[i]} = \left[ \begin{array}{cccc} | & | & & | \\ a^{[i](1)} & a^{[i](2)} & \cdots & a^{[i](m)} \\ |
& | & & | \end{array} \right] \]

A 、Z 的水平方向表示的是不同的样本，垂直方向表示的不同网络某一层中的不同节点。

\[ Z^{[i]} = W[i]A[i-1] + b^{[i]}\]
\[ A^{[i]} = sigmoid(Z^{[i]}) \]

如果把输入按列堆叠，输出也将按列堆叠。

**** Activation function
激活函数，不同网络层的激活函数可以不同。

\begin{align*}
sigmoid(z) & = \frac{1}{ 1-e^{-z} } \\
tanh(z) & = \frac{ e^z - e^{-z} }{ e^z + e^{-z} }, \quad \text{a shifted version of sigmoid} \\
ReLU(z) & = \max(0,z) \\
leaky ReLU(z) & = \max(0.01z,z) \\
& = \left\{ \begin{array}{} 0.01z, & z < 0 \\ z, & z \geq 0 \end{array} \right.
\end{align*}

tanh 函数几乎总是比 死规模的 sigmoid 函数的效果要好，因为其均值为 0 ？更有利于后面网络层的学习，类似于将输入样本的均值归
一化到 0 一样。网络的输出层（二分类）可以使用 sigmoid 函数，其他的时候几乎不要使用。但是两者当 z 很大或者很小的时候，两
者的梯度变得很小，将减慢网络的学习速度。深度神经网络中，一般都只使用 ReLU 激活函数。另外 leaky ReLU 理论上效果会更好，不
会一半的导数为 0 ，但实际上很少使用。

*为什么需要激活函数：* 如果没有激活函数或者激活函数是线性的，那么无论网络层数有多少，其实际上都只是在做线性回归。两个或
者多个线性函数的叠加仍然是线性函数。所以使用非线性的激活函数非常重要。

*激活函数求导：* 

\begin{align*}
g(z) & = \frac{1}{1 + e^{-z}} \\
g'(z) & = g(z)(1-g(z)), \quad \text{compute quickly when g(z) is know} \\
g(z) & = tanh(z) \\
g'(z) & = 1-(tanh(z))^2 \\
g(z) & = max(0,z) \\
g'(z) & = \left\{ \begin{array}{} 0 & if \ z < 0 \\ 1 & if \ z > 0 \\ undefined & if \ z = 0 \end{array} \right. \\
g(z) & = max(0.01z,z) \\
g'(z) & = \left\{ \begin{array}{} 0.01 & if \ z < 0 \\ 1 & if \ z > 0 \\ undefined & if \ z = 0 \end{array} \right.
\end{align*}

实际中使用 ReLU 或者 leaky ReLU 函数时，z 为 0 的概率很小很小，所以使用时，让激活函数的导数在 0 点等于 0 或者 1 都可以，
并不会影响结果。

**** 反向传播

利用计算图(computation graph)表示前向传播和反向传播。前向传播时，需要计算网络的输出，每经过一个节点，都需要乘以该节点的
函数表达式来得到该神经元的输出值，然后继续向前传播；反向传播时，需要计算的是参数的导数，根据导数的链式法则，每经过一个神
经元，都需要乘以该神经元函数表达式对需要求导变量的偏导数，然后继续反向传播。无论前向传播还是反向传播，都是一层一层的计算，
根据前一层的结果来求得本层节点的值，只是前向传播时乘以的是节点的函数表达式，而反向传播时乘以的是偏导数。

比如求取导数 dz ，并且已知 \(a = g(z)\)
\[ dz = da \cdot g'(z) \]

前向传播过程中计算激活函数时，是将矩阵中的每个元素都乘以激活函数的表达式，也就是逐元素相乘；反向传播乘以激活函数的导数时
也要逐元素相乘。

无论前向传播还是反向传播，计算过程中确保矩阵的维数相同，将避免很多问题。

权重的维数
\[ W^{[l]}.shape = (n^{[l]},n^{[l-1]}) \]
\[ Z^{[l]}.shape = (n^{[l]}) \]

某个向量 v 和其导数 dv 的维数必定总是相同的。

**** 随机初始化权重

在深度学习中，必须使用随机初始化的方式来初始化权重。假如将所有的权重都初始化为 0，那么前向传播时，由于对称性每个神经元节
点的值都会相同，反向传播时，得到的每个神经元节点权重的导数也相同，从而导致所有神经元的权重都是相同的。而实际上我们希望不
同的神经元使用不同的权重，来计算不同的特征。这将导致神经网络无法工作。

#+BEGIN_SRC python
# 通常把权重初始化称非常小的随机数；防止直接达到 sigmoid 函数梯度很小的地方
W1 = np.random.randn(2,2) * 0.01 #
b1 = np.zeros(2,1) # 无需随机初始化
#+END_SRC

*** 深层神经元网络

算法的复杂性来自于数据而不是代码，所以很多时候会惊讶，这么简单的代码居然实现了这么 6 的功能。

发现一些问题只有深层网络可以求解，浅层网络无法解决。下面有两个解释使用深层网络的原因：

1. 使用深层网络检测人脸，开始的网络层检测的是脸部的边缘（横线、竖线、不同角度的斜线），之后的网络层检测的是五官（由浅层
   网络组合成眼睛、鼻子、嘴巴等部位），随后的网络层组合不同的五官来组成整张脸从而识别身份。语音类似
2. 复杂的数学函数，如果使用多层网络来学习表示，那么每个节点只需要是一个很简单的函数，将这些简单的函数组成一个深层网络，
   就可以很好的表示该复杂函数；而如果只使用一个隐层，那么隐层函数将会非常复杂，可能需要指数级个数的节点。

逐样本计算公式：

\begin{align*}
z^{[l]} & = W^{[l]} a^{[l-1]} + b^{[l]} \\
a^{[l]} & = g^{[l]} (z^{[l]} \\

dz^{[l]} & = da^{[l]} * {g^{[l]}}' (z^{[l]}) \quad \text{element wise product} \\
dW^{[l]} & = dz^{[l]} \cdot a^{[l-1]} \\
db^{[l]} & = dz^{[l]} \\
da^{[l-1]} & = {W^{[l]}}^T \cdot dz^{[l]}
\end{align*}

向量化计算所有样本公式：

\begin{align*}
Z^{[l]} & = W^{[l]} A^{[l-1]} + b^{[l]} \\
A^{[l]} & = g^{[l]} (Z^{[l]}) \\

dZ^{[l]} & = dA^{[l]} * {g^{[l]}}' (Z^{[l]}) \quad \text{element wise} \\
dW^{[l]} & = dZ^{[l]} \cdot {A^{[l-1]}}^T \\
dB^{[l]} & = \frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True) \\
dA^{[l-1]} & = {W^{[l]}}^T \cdot dZ^{[l]}
\end{align*}


#+BEGIN_SRC python
# Talk is cheap, show me the code
import numpy as np
# ReLU 激活函数
def ReLU(Z):
    # ReLU(z) = max(0,z)

    # numpy broadcoast
    res = np.maximum(Z, 0)

    return res

# ReLU 激活函数的导数
def dReLU(Z):
    # 所有小于 0 的值导数为 0
    dZ = np.maximum(Z, 0)
    # 所有大于 0 的值导数为 1
    dZ[Z > 0] = 1

    return dZ

def forword(Apl, Wl, bl, g):
    '''
    Apl: 上一层的节点的输出
    Wl: 本层节点的权重
    bl: 本层节点的偏移
    g: 本层网络的激活函数

    '''

    # Apl.shape[1] = bl.shape[1] = minibatch size
    assert(bl.shape[1] == 1 or Apl.shape[1] == bl.shape[1])
    # Wl.shape = (Al.shape[0], Apl.shape[0])
    assert(Apl.shape[0] == Wl.shape[1])
    # A 的行数等于 W 的行数，列数等于 Apl 的列数

    # Z^{[l]} & = W^{[l]} A^{[l-1]} + b^{[l]}
    # A^{[l]} & = g^{[l]} (Z^{[l]})
    Zl = np.dot(Wl, Apl) + bl
    Al = g(Zl)

    return Al,Zl


def backword(dAl, Wl, Zl, dgl, Apl):
    '''
    dAl: 本层节点的导数
    Wl: 本层节点的权重
    Zl: 本层节点激活函数的输入
    dgl: 本层网络激活函数的导函数
    Apl: 本层节点的输出
    '''

    assert(dAl.shape == Zl.shape)
    assert(Wl.shape == (dAl.shape[0], Apl.shape[0]))
    assert(dAl.shape[1] == Apl.shape[1])

    # dZ^{[l]} & = dA^{[l]} * {g^{[l]}}' (Z^{[l]}) \quad \text{element wise product} \\
    # dW^{[l]} & = dZ^{[l]} \cdot {A^{[l-1]}}^T \\
    # dB^{[l]} & = \frac{1}{m} np.sum(dZ^{[l]}, axis=1, keepdims=True) \\
    # dA^{[l-1]} & = {W^{[l]}}^T \cdot dZ^{[l]}
    dZl = np.multiply(dAl, dgl(Zl)) # element wise product
    dWl = np.dot(dZl, Apl.transpose())
    dbl = np.sum(dZl, axis = 1, keepdims = True) / dAl.shape[1]
    dApl = np.dot(Wl.transpose(), dZl)

    return dApl,dWl,dbl


Apl = np.random.randn(5,8)
Wl = np.random.randn(9,5) * 0.01
bl = np.random.randn(9,1)

Al,Zl = forword(Apl, Wl, bl, ReLU)
print("Al")
print(Al)

print("Zl")
print(Zl)

dAl = np.random.randn(9,8)
dApl,dWl,dbl = backword(dAl, Wl, Zl, dReLU, Apl)

print("dApl")
print(dApl)
print("dWl")
print(dWl)
print("dbl")
print(dbl)

#+END_SRC


**** 核对矩阵的维数

拿出纸和笔，手算一下每个矩阵的维数，可以大大减小网络的 bug 。

| 参数         | 维数                     |
|--------------+--------------------------|
| \(W^{[l]}\)  | \((n^{[l]}, n^{[l-1]})\) |
| \(dW^{[l]}\) | \((n^{[l]}, n^{[l-1]})\) |
| \(b^{[l]}\)  | \((n^{[l]}, 1)\)         |
| \(B^{[l]}\)  | \((n^{[l]}, m)\)         |
| \(db^{[l]}\) | \((n^{[l]}, 1)\)         |
| \(dB^{[l]}\) | \((n^{[l]}, m)\)         |
| \(z^{[l]}\)  | \((n^{[l]}, 1)\)         |
| \(Z^{[l]}\)  | \((n^{[l]}, m)\)         |
| \(a^{[l]}\)  | \((n^{[l]}, 1)\)         |
| \(A^{[l]}\)  | \((n^{[l]}, m)\)         |

无论是否向量化同时计算多个样本，权重 W 的维数都是一样的。

**** Hyperparamter

*超参：* 学习速率、迭代次数、隐层数、每一层节点的个数、激活函数、minibatch size、momentum、regularization parameters

这些超参需要手动设置，并且这些参数经影响你参数的最终结果。而预先很难知道最优的超参是什么，所以必须尝试各种参数
（依据 idea->code->experiment 循环），观察模式是否成功。并且可能由于电脑环境 CPU GPU 老化或者其他原因，最优超参也是会不
断变化，每隔一段时间需要重新调节超参。

凭经验的过程通俗的来说就是不断尝试直到找到合适的数值。
empirical process is maybe a fancy way of saying you just have to try a lot of things and see what works.

深度学习用于计算机视觉、语音、自然语言处理、广告投放、搜索、数据分析等。深度学习应用到了很多结构化的数据分析中。


** 提升深度神经元网络：超参调节、正则化、最优化

深度学习中有很多的超参，我们不可能一开始就是知道这些超参的最优解。应用机器学习的过程是一个高度迭代的过程：在项目启动的时
候，我们有一个初步想法（对超参的一个设置），然后运行代码进行实验，根据结果去改变策略或者完善想法，从而不断找到更加优化的
网络。深度学习已经应用到了各个领域，经常有某个领域的专家投身到其他领域中去，然而不同领域对超参设置的直觉、经验通常并不适
合其他的领域。最佳的选择通常依赖于你的数据量、输入特征的数量、计算机的配置（GPU群、单GPU、CPU）。所以即使是专家也通常无
法开始就知道超参的准确值，深度学习是一个典型的迭代的过程，通过不断的验证来提高网络的性能。所以项目的进度直接依赖于每一个
迭代的时间，设置高质量的训练、验证、测试集可以提高迭代的效率。

*** Training - Development - Test Data Set

正确选择训练集、验证集[Hold-out cross validation]、测试集可以很大程度上帮助我们创建一个高效的神经网络。

在样本较少的机器学习时候，普遍认为最好的比例为 70/30 的训练集和交叉验证集，或者 60/20/20 的训练集、交叉验证集、测试集。
在深度学习中，一般都有海量的数据，此时验证集和测试集的比例会变得很小。因为验证集目的是验证不同算法的优劣，所以验证集只需
要拥有能够验证那个算法更好的个数的样本就可以。测试集的目的是评估分类器的性能，同样并不需要 20% 的数据去评估。并且可以没
有测试集，因为测试集是为了得到网络性能的无偏估计，当不需要网络的无偏估计的时候可以不需要测试集。

100万 ： 98/1/1, 数据量更大时：99.5/0.25/0.25, 99.5.0.4/0.1

训练集和测试集分布不同： *确保验证集和测试来自相同的分布。* 利用爬虫等从网络上获取训练图片，可能使得网络的训练集和测试集
分布不同，但是一定要让验证集和测试集的分布相同，这样可以让机器学习算法收敛的更快。

*** Bias and Variance

偏差和方差两个概念很容易学，但很难理解。即使你认为已经学会了两者的基本概念，不过总是有一些意想不到的新东西出现。

在深度学习中，不再需要权衡(trade-off)偏差和方差。因为现在有方法可以只较小偏差而对方差的影响很小，或者只减小方差而对偏差
的影响很小，不像原来那样减小其中一个势必增大另一个。

在二维时可以通过画图达到可视化的效果来观察偏差和方差；在高维空间中可以通过训练误差和验证误差两者来观察偏差和方差。

| 训练集误差 | 验证集误差 | 偏差-方差[贝叶斯误差接近 0%，训练样本和验证样本同分布] |
|------------+------------+--------------------------------------------------------|
|         1% |        11% | 高方差（过拟合）                                       |
|        15% |        16% | 高偏差（欠拟合）                                       |
|        15% |        30% | 高偏差和高方差                                         |
|       0.5% |         1% | 低偏差和低方差                                         |

同时高偏差和高方差的情况：在高维空间中，有些区域偏差高、有些区域方差高。

偏差比较高的时候，如果去寻找更多的训练样本来训练网络，通常帮助不大，且会浪费时间。所以一定要清楚系统现在是高偏差还是高方
差，从而使用更加精确的方法来改善系统。

调试系统的基本方法：
1. 首先查看系统是否是高偏差。根据人眼的识别率来近似估计贝叶斯误差；如果系统的偏差较大，可以通过训练更大的网络（增加网络
   的层数或者隐层节点的个数）、增长训练时间、改善系统的网络架构等方法来减小偏差，直到将偏差降低到一个合理的范围。
2. 然后依据偏差的大小查看系统的是否是高方差。如果系统是高偏差，可以通过使用更多的训练样本、正则化、不同的网络架构等方法
   来改善偏差。
3. 如果需要再进入第一步，直到训练出一个合理的系统。

训练一个正则化的更大的网络几乎没有任何负面影响，只是会增长训练时间，需要更大的训练样本。

*** 正则化

如果怀疑网络出现了过拟合，首先应该考虑正则化，当然使用更多的训练样本同样可以减小过拟合，但有时候可能不现实。

square Euclidean norm 欧几里德范数的平方

正则化的时候只考虑权重 w ，而不考虑 b ，是因为 w 包含了绝大多数参数，而 b 只有很少的参数，影响不大。当然如果需要同样可以
在正则化项中增加 b 。

\begin{align*}
J(W^{[1]}, b^{[1]}, \cdots , W^{[L]}, b^{[L]}) & = \frac{1}{m} \sum_{i=1}^m L({\hat{y}}^{(i)} - y^{(i)}) 
\color{red}{ + \frac{\lambda}{2m} \sum_{l=1}^{L} ||W^{[l]}||_F^2 } \\
dW^{[l]} & = dZ^{[l]} \cdot {A^{[l-1]}}^T \color{red}{ + \frac{\lambda}{m} W^{[l]} } \\
W^{[l]} & := W^{[l]} + \alpha dW^{[l]} \\
& = W^{[l]} - \alpha ( dZ^{[l]} \cdot {A^{[l-1]}}^T \color{red}{ + \frac{\lambda}{m} W^{[l]} } ) \\
& = (\color{green}{1 - \frac{\alpha \lambda}{m}}) W^{[l]} - \alpha ( dZ^{[l]} \cdot {A^{[l-1]}}^T )
\end{align*}

由于 \(1 - \frac{\alpha \lambda}{m} < 0\) ，L2 正则化也称为权重衰减(weight decay)。
L2 正则化使用较广泛。Frobenius norm

\[  ||W^{[l]}||_F^2 = \sum_{i=1}^{n_l} \sum_{j=1}^{n_{l-1}} w_{ij}^{[l]} \]

L1 正则化可以使权重变得稀疏，也就是会使权重中存在较多的 0 。吴恩达认为虽然有较多的权重参数为 0，但是对减少存储空间没有太
大的贡献。

lambda 是正则化参数，通过交叉验证来选择，从而使得训练误差和权重参数之和最小，来减小过拟合的风险。

lambda 是 Python 的一个保留关键字，编程时使用 lambd 来代替表示正则化参数。

*直观理解正则化可以减小方差：* 增加正则化项，假如正则化参数 labda 很大，那么将有很多的权重参数变得几乎为 0，从而消除或者
减小了中间网络层节点对结果的影响，从而使得网络变得简单。从而不容易产生过拟合。逐渐减小正则化参数，可以找到一个合适的值使
得网络偏差和方差都不是很大。

Dropout（随机失活）是一个非常有效的正则化方法。常用 inverted dropout，只在训练阶段使用 dropout，在测试阶段不使用 dropout。

#+BEGIN_SRC python
# 每次训练的时候 dropout 的网络节点不相同，都是随机的
# inverted dropout
keep_prob = 0.5
# 反向传播的时候仍然使用该矩阵
dropout3 = np.random.randn(a3.shape) < keep_prob
a3 = np.multiply(a3, dropout3)
a3 /= keep_prob
#+END_SRC

*直观理解 Dropout ：* Dropout 使得网络结构变得简单，从而减少了过拟合；由于会随机丢弃一些节点，所以一个神经元就不能够依赖
其某一个或者某几个固定的输入节点，而是会将权重分散开来到每一个输入节点，相当于 shrink 了权重，所以使得权重参数的 F 范数
变少，达到了类似 L2 正则化的效果。

可以在不同的网络层使用不同大小的 keep_prob ，在含有较多权重参数的网络层，使用较小的 keep_prob （如 0.5），从而预防该层网
络过拟合；在含有较少权重参数的网络层，使用较大的 keep_prob （如 0.7 、0.9），因为不用太担心该层网络会过拟合。输入层一般
不使用 Dropout ，即让 keep_prob 等于 1 ，或者很接近 1 的某个值。当然让不同的网络层有不同的 keep_prob 增加了超参的个数，
需要使用交叉验证来选择参数。

Dropout 使得代价函数 J 的定义变得不明确，因为每次都会随机丢弃一些节点。所以在最开始训练的时候可以先关掉 Dropout ，使网络
所有层的 keep_prob = 1 ，观察代价函数 J 是否会随着迭代次数的增加而减小，从而减小因为引入 Dropout 而导致的 bug 。然后再打
开 Dropout 开始训练网络。

记住：Dropout 是为了防止网络过拟合的一种正则化方法，除非确认网络会过拟合，否则不要使用。当然 Dropout 在图像中使用很频繁，
因为有太多的参数，以至于总是没有足够数量的样本，所以才会默认都使用 Dropout。

data argumentation: 通过水平翻转(flipping horizontally)、随机裁剪(random crops)[原图随意旋转放大后再裁剪]等方法来扩大数
据集。这样数据集会有冗余，虽然不如使用全新样本效果好，但是节省了寻找新样本的时间。注意：需要经过处理后的样本仍然保持基本
模样，如可以将一张猫的图片水平翻转，但是不要上下翻转，那样猫将上下颠倒（我怎么感觉也需要上下翻转，因为有时候很有可能看到
的就是一个上下颠倒的猫）。对于光学字符识别，可以通过任意的旋转和扭曲来扩张数据。

early stopping：通过不断的迭代，训练误差不不断减小，但是验证误差会在减小到一定值之后开始增加，early stopping 就是希望在
验证误差比较小的时候停止。另外由于权重初始化为很小的值，随着迭代次数的不断增加，权重变得越来越大，early stopping 在权重
不是很大的时候停下了，就类似与 L2 正则化的效果。缺点：early stopping 会同时调节损失函数和正则化两者，不符合正交化的规则，
可能使得两者调节的都不好。使用 L2 正则化则可以让网络的迭代次数尽可能多，而无需考虑过拟合，只是需要多次验证最优的正则化参
数。

*正则化输入：* 将输入样本的均值和方差归一化将有助于提高网络的训练速度。因为假如样本的不同特征的范围差别很大（特征 1 的范
围是 0-1 ，特征 2 的范围是 1-1000）将会让损失函数的形状类似与一个细长形状，contour 是细长的椭圆。必须使用很小的学习速率
来反复学习（否则将远离最优解），势必需要耗费很多时间。而将特征归一化处理之后，损失函数将是一个圆碗的形状，其 contour 是
圆形，可以快速收敛。样本归一化只有在样本的不同特征范围差别很大的时候才会生效，但使用不会有什么坏处，所以可以总是使用。样
本正则化共需要两步：均值转换成 0 ，方差转换成 1 。

1. 让 \(\mu = \frac{1}{m} \sum_{i=1}^m x^{(i)}\)
2. 使用 \(x^{(i)} - \mu\) 逐一替换 \(x^{(i)}\) ；这两步用于将均值变换成 0，若已知均值为 0 ，可跳过此步骤
3. 让 \(\sigma_j^2 = \frac{1}{m} \sum_i (x_j^{(i)})^2 \) ；逐元素求平方
4. 使用 \(x_j^{(i)} / \sigma_j\) 逐一替换 \(x_j^{(i)}\) ；将协方差变为单位阵，方差归一化使得不同的属性拥有相同的尺度。

*在测试集中仍然需要使用训练集的 \(\mu\) \(\sigma\) 参数，不可以让测试集去使用自己的参数*

*** vanishing / exploding gradient

梯度消失/爆炸：当网络的层数很深的时候，如果所有的权重都大于 1 ，那么最终节点的输出值将变得很大；如果所有权重都小于 1 ，
那么最终的输出值将变得很小。从而出现梯度爆炸或者消失的问题。可以通过合适的选择权重初始化的值来缓解这个问题，让所有节点的
输出值都在 1 的附近，从而不会很快的爆炸或者消失。同样是一个加速训练网络的方法。

#+BEGIN_SRC python
# hurd 论文公式，适用 ReLU 激活函数
Wl = np.random.randon(nl, npl) * np.sqrt(2 / npl)
# 可以将 2 视为一个超参来调节，但其优先级较低
#+END_SRC

*** Gradient check

使用梯度检查有利于查找代码中的 bug 。方法：将所有的权重参数 \(W^{[1]},b^{[1]},\cdots,W^{[L]},b^{[l]}\) 都变换成列向量，
然后串接成一个大向量 \(\theta\) ，其中每一个列向量记为 \(\theta_1,\theta_2,\cdots,\theta_{2L}\) 。同时将所有的梯度向量
\(dW^{[1]},db^{[1]},\cdots,dW^{[L]},db^{[l]}\) 转换成列向量并串接成一个向量 \(d\theta\) 。使用 for 循环变量大向量
\(\theta\) 的每一个小向量 \(\theta_i\) ，计算 \[ d\theta_{approx} [i] = \frac{J(\theta_1,\theta_2,\cdots,\theta_i +
\varepsilon, \cdots, \theta_{2L}) - J(\theta_1,\theta_2,\cdots,\theta_i - \varepsilon, \cdots,
\theta_{2L})}{2\varepsilon} \] 然后比较两个向量 \(d\theta_{approx},d\theta\) 的相似度 \[ \frac{||d\theta_{approx} -
d\theta||_2}{||d\theta_{approx}||_2 + ||d\theta||_2} \] 通常选取 \(\varepsilon = 10^{-7}\) ，查看两个向量的相似度如果也
在 \(10^{-7}\) 表明没有问题，若在 \(10^{-5}\) 则可能有问题，更大的话则肯定有问题。这里使用的是双边检查，相比于单边检查更
加精确。另外有几点需要注意：

+ 只在 debug 的时候使用双边检查。训练网络的时候不要使用，否则会减慢训练速度
+ 如果检查有问题，可以通过比较两个大向量的差别比较大的 i 来进一步定位问题的位置
+ 如果使用了正则化，记得在代价函数和梯度中都有相应的增加项
+ 不使用 Dropout
+ 在网络训练过一段时候后，再次检查一下；可能网络只在权重比较小的时候是正确的

\begin{gather*}
f'(\theta) = \lim_{\varepsilon \to 0} \frac{f(\theta + \varepsilon) - f(\theta - \varepsilon)}{2\varepsilon}. \quad 
error \ O(\varepsilon^2) \\
f'(\theta) = \lim_{\varepsilon \to 0} \frac{f(\theta + \varepsilon) - f(\theta)}{\varepsilon}. \quad 
error \ O(\varepsilon) \\
\end{gather*}

O(n): on the Order of 。表示常数乘以括号中的项

** 让机器学习工程变得结构化

*** mini-batch gradient descent

当训练数据即非常大的时候（比如有 500 万个样本），使用批量梯度下降法将非常的耗时，因为必须要计算所有的样本后才可以调节参
数。因此使用 mini-batch 梯度下降法结合了随机梯度下降法和批量梯度下降法两者的优点：可以使用向量话计算同时避免必须计算完所
有样本后才能更新参数。

将样本特征和标记都分成许多等个数的小段，记为 \(X^{\{t\}}, Y^{\{t\}}\) 。计算过程同批量梯度下降法类似，每一次 mini-batch
迭代称为一个 epoch 。

mini-batch size 是一个很重的超参，使用时需要快速选择。使用较大的是 64-512 之间的某个 2 的次方数。并且确保 mini-batch
size fit in 你的 CPU / GPU 的内存。

*** Momentum

通常选取 momentum 参数 \(\beta = 0.9\) ，这是一个比较鲁棒的值。

\begin{align*}
v_{dW} & = \beta v_{dW} + (1 - \beta)dW \\
v_{db} & = \beta v_{db} + (1 - \beta)db \\

W & = W - \alpha v_{dW} \\
b & = b - \alpha v_{db}
\end{align*}

#+BEGIN_SRC python
vdW = np.zeros(dW.shape)
vdb = np.zeros(db.shape)
# 第 t 次迭代，计算 mini-batch 的 dW db
vdW = beta * vdW + (1 - beta)dW
vdb = beta * vdb + (1 - beta)db

W = W - alpha * vdW
b = b - alpha * vdb
#+END_SRC

计算得到本次的导数值之后，使用指数加权移动平均来估计过去 10 次迭代的平均值，然后使用平均后的权重值来更新权重。这样将使得
迭代左右来回摆动得到抑制（细长型的代价函数，每次迭代都会左右摆动，通过计算过去 10 次的平均值，正负得到抵消，左右摆动将会
被消除很多，相当于给其增加了摩擦力，估计也是称为 momentum 的原因），而使用向最优解移动的方向则不会被抑制。由于仅需经过
10 次迭代之后就可以消除因为初始化为 0 带来的偏差，通常不需要偏差修正。


**** Exponentially Weighted Moving Averages

\[ V_t = \beta V_{t-1} + (1 - \beta) \theta_t \] 指数加权移动平均，大约相当于求取了 \(\frac{1}{1-\beta}\) 个数的平均值
\( (1-\varepsilon)^{\frac{1}{\varepsilon}} = \frac{1}{\varepsilon}\) 。当 \(\beta\) 较小的时候（比如等于 0.5），平均的数
量较小，会对当前值有快速的响应，但也会有较大的震荡；当 \(\beta\) 较大的时候（比如等于 0.99），求取了太多数量的平均值，导
致对当前数值不敏感，最终的曲线会有延后。使用时作为超参来调节。

实际使用时只需要先将 V 初始化成 0，然后有新的值时使用公式 \(V := \beta V + (1-\beta) \theta_i\) 更新 V 即可。这样只需要
占用一个内存，也很高效；虽然计算并不精确，如果时刻记录过去 50 个的值，然后求和在求平均计算更精确，但比较繁琐，且耗内存。
Bias correction，由于将 V 初始化成 0，导致最初的估计会存在偏差。可以在求得 V 之后再除以一个修正变量
\(\frac{V}{1-\beta^t}\) 来代替 \(V\) ，就可以修正因初始值估计不准确而导致的偏差。

*** RMSprop

root mean square prop

\begin{align*}
s_{dW} & = \beta_2 s_{dW} + (1 - \beta_2) dW^2 \\
s_{db} & = \beta_2 s_{db} + (1 - \beta_2) db^2 \\
W & := W - \alpha \frac{dW}{\sqrt{s_{dW} + \varepsilon}} \\
b & := b - \alpha \frac{db}{\sqrt{s_{db} + \varepsilon}}
\end{align*}

#+BEGIN_SRC python
# 第 t 次迭代，计算出 dW db 后
SdW = beta2*Sdw + (1 - beta2)dW**2 # 逐元素求平凡
Sdb = beta2*Sdb + (1 - beta2)db**2

W = W - alpha * dW / np.sqrt(SdW + epsilon) # epsilon 是一个很小的值，防止除以 0
b = b - alpha * db / np.sqrt(Sdb + epsilon) # epsilon 可取 10^(-8)
#+END_SRC

*** Adam

Adaptive momentum estimation 结合了 Momentum 和 RMSProp 两个算法，需要偏差修正。

\begin{align*}

\end{align*}

超参选择：
+ \(\alpha\) 需要调节
+ \(\beta1 = 0.9\) 
+ \(\beta2 = 0.999\)
+ \(\varepsilon = 10^{-8}\)

** 卷积神经网络


** 自然语言处理


