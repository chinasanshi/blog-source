#+TITLE:       machine learning
#+AUTHOR:      Kyle Three Stones
#+DATE:        <2018-06-22 Fri 07:19>
#+EMAIL:       kyleemail@163.com
#+OPTIONS:     H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t f:t tex:t
#+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler
#+STARTUP: latexpreview
#+TAGS:        机器学习, 
#+CATEGORIES:  机器学习

* ML
*机器学习* machine learning: Field of study that gives computers the ability to learn without being explicitly program.

机器学习并不是仅仅是若干算法的堆积，学会“十大算法”，熟练掌握具体算法算法的推导与编程实现，并不能让所有问题迎刃而解，因为
现实世界的问题千变万化。而应该像张无忌那样，忘记张三丰传授的太极剑法的具体招式，而只记住一些规则和套路，从而根据敌人的招
式去不断变化自己的招式，达到以不变应万变的效果。或者说用 Andrew Ng 的话，要成为一个 master carpenter （顶级木匠），可以
灵活使用工具来制造桌椅，只有手艺差的木匠才会抱怨工具不合适。因此必须把握算法背后的思想脉络，针对具体的任务特点，对现有套
路进行改造融通；要记住算法是 *死* 的，思想才是 *活* 的。

数据库提供数据管理技术，机器学习提供数据分析技术。

** Supervised Learning
监督学习

*** Support Vector Machines - SVM
*SVM 的基本思想就是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。* 

先写算法的最终求解方法，步骤。认为此时已经知晓所有知识。

**** 算法步骤
1. 构造含约束的最优化问题；根据 KKT 条件求得分离超平面的表达式
2. 求得相应的对偶问题
3. 利用 SMO 算法高效求解对偶问题，得到分离超平面的参数值
4. 依据分离超平面来处理新样本

再写一些术语的解释

**** 算法演进过程：

\begin{align}
max_{w, b} \quad & \gamma \\
s.t. \quad & y^{(i)} \left( \frac{w}{||w||} \cdot x^{(i)} + \frac{b}{||w||} \right) 
\geq \gamma, \quad i=1,2,\ldots,m
\end{align}

\begin{align}
max_{w, b} \quad & \frac{\hat{\gamma}}{||w||} \\
s.t. \quad & y^{(i)} \left( w \cdot x^{(i)} + b \right) \geq \hat{\gamma}, \quad i=1,2,\ldots,m
\end{align}

\begin{align}
min_{w, b} \quad & \frac{1}{2}||w||^2 \\
s.t. \quad & y^{(i)} \left( w \cdot x^{(i)} + b \right) \geq 1, \quad i=1,2,\ldots,m
\end{align}

\begin{align}
min_{w, b, \color{red}{\xi_i}} \quad & \frac{1}{2}||w||^2 + C\sum_{i=1}^m \xi_i \\
s.t. \quad & y^{(i)} \left( w \cdot x^{(i)} + b \right) \geq 1 - \xi_i, \quad i=1,2,\ldots,m \\
& \xi_i \geq 0, \quad i=1,2,\ldots,m
\end{align}

\begin{align}
max_{\alpha} \quad & W(\alpha) = \sum_{i=1}^m \alpha_i - 
\frac{1}{2} \sum_{i,j=1}^m y^{(i)}y^{(j)} \alpha_i\alpha_j \left \langle x^{(i)},x^{(j)} \right \rangle \\
s.t. \quad & 0 \leq \alpha_i \leq C, \quad i = 1,2,\ldots,m \\
& \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align}

\begin{align}
max_{\alpha} \quad & W(\alpha) = \sum_{i=1}^m \alpha_i - 
\frac{1}{2} \sum_{i,j=1}^m y^{(i)}y^{(j)} \alpha_i\alpha_j K( x^{(i)},x^{(j)} ) \\
s.t. \quad & 0 \leq \alpha_i \leq C, \quad i = 1,2,\ldots,m \\
& \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align}

\begin{align}
w^* & = \sum_{i=1}^m \alpha_i^* y^{(i)} x^{(i)} \\
b^* & = y^{(i)} - \sum_{i=1}^m \alpha_i^* y^{(i)} K(x^{(i)}, x^{(j)}) \\
f(x) & = sign \left( \sum_{i=1}^m \alpha_i^* y^{(i)} K(x \cdot x^{(i)}) + b^* \right)
\end{align}

1. 因为 SVM 学习算法的目标是最大化几何间隔\(\gamma\)，所以构建相应的模型，其目标函数表示为最大化几何间隔，同时约束每个训
   练样本距离分离超平面的距离大于该几何间隔。
2. 由于最终需要求解的是分离超平面的权重，所以需要利用几何间隔与函数间隔的关系，让目标函数中显示出现分离超平面的权重；即
   将目标函数和约束中的几何间隔统一改为函数间隔 \(\hat{\gamma}\)。
3. 函数间隔的取值并不影响上述最优化问题的解（当分离超平面的权重成比例变化时，函数间隔也呈现相应比例的变化），即函数间隔
   成比例变化时对不等式约束没有影响（相当于不等式的两边同时乘以该变化系数），对目标函数的优化也没有影响（目标函数的分子
   和分母同时乘以该变化系数）。为了简化模型表达式，取函数间隔为 *1* 。
4. 此时目标函数的分子为 1 ，分母为分离超平面的权重，且为求最大值。由于权重处于分母时，不利于求解，此时只需要最小化目标函
   数的分母即可。同时将权重转化为二次。此时的约束最优化问题与原问题是等价的，这是一个凸二次规划问题（convex quadratic
   programming）。Andrew Ng 讲解上述的变化都是在将非凸优化问题转化成凸优化问题。
5. 由于存在线性不可分以及 outliers 让分离超平面变差的可能。使函数间隔不再始终大于1，将函数间隔减去一个松弛变量，即函数间
   隔大于\(1-\xi_i\)。由于不再要求函数间隔始终大于1，所以可以找打一个分离超平面来分割不同的类别。但同时也不希望松弛变量
   太大，所以在目标函数中增加相应的正则化项（这里使用的是\(\ell_1\) regularization）。同时使用C来调节权重与松弛变量在目
   标含住中的比例关系。
6. 由于该凸二次规划问题求解时间复杂度较高，所以转而去求解相应的对偶问题。使用对偶问题可以高效求解，同时可以使用核函数。
7. 如果可以将特征都转换成內积的表示形式，就可以使用核函数。转换成內积形式以后，将表达式中的\(x\)转换成\(\phi(x)\) 就表示
   使用了核函数。而且并不需要知道\(\phi(x)\)的具体表达式，只需要代入\(\phi(x)^T \phi(z)\)乘积的结果，即选取的核函数的表
   达式\(K(\phi(x),\phi(z))\)就可以，这样将大大简化计算。
8. 另外转换成对偶问题后，可以使用 SMO（sequential minimal optimization）算法来高效求解参数\(\alpha_1,\ldots,\alpha_m\)

**** 术语解释
***** 原问题 - 对偶问题
虽然有很多的最优化算法可以求解该凸二次规划问题，但是当训练样本容量很大时，这些算法往往变得非常低效。而当参数满足
Karush-Kuhn-Tucker (KKT) 条件时，原问题和对偶问题的解相同；并且将原问题转换成对偶问题后，可以使用 SMO 算法高效求解，所以
才提出了原问题(primal program)及对偶问题(dual problem)。事实上这里使用的是拉格朗日对偶性(Lagrange duality)，原问题和对偶
问题的函数表达式都是拉格朗日函数(Lagrange function)。我们需要解决的问题一般都是含约束的最优化问题，而求解含约束的最优化
问题，一种有效的方法就是使用拉格朗日乘数法。使用拉格朗日乘数法首先构造拉格朗日函数，然后对拉格朗日函数参数的不同求解顺序
构成了原问题及对偶问题。

\begin{align}
min_{w} \quad & f(w) \\
s.t. \quad & g_i (w) \leq 0, \quad i=1,2,\ldots,k \\
& h_i (w) = 0, \quad i=1,2,\ldots,l
\end{align}

\begin{equation}
\mathcal{L}(w,\alpha,\beta) = f(w)+ \sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w)
\end{equation}

\begin{align}
\theta_p(w) & = max_{\alpha,\beta:\alpha_i \geq 0} \ \mathcal{L}(w, \alpha, \beta) \\
min_w \theta_p(w) & = min_w max_{\alpha,\beta:\alpha_i \geq 0} \ L(w, \alpha, \beta) \\
p^* & = min_w \ \theta_p(w) 
\end{align}

\begin{align}
\theta_D(\alpha, \beta) & = min_w \ \mathcal{L}(w, \alpha, \beta) \\

max_{\alpha,\beta:\alpha_i \geq 0} \ \theta_D(\alpha, \beta) & = max_{\alpha,\beta:\alpha_i \geq 0}\ 
min_w \ \mathcal{L}(w, \alpha, \beta) \\

d^* & = max_{\alpha,\beta:\alpha_i \geq 0} \ \theta_D(\alpha, \beta)
\end{align}

\begin{equation}
d^* = max_{\alpha,\beta:\alpha_i \geq 0}\ min_w \ \mathcal{L}(w, \alpha, \beta) \leq
min_w max_{\alpha,\beta:\alpha_i \geq 0} \ \mathcal{L}(w, \alpha, \beta) = p^*
\end{equation}

将原始的问题转换成拉格朗日函数，其中\(\alpha_i\) 和 \(\beta_i\)都是拉格朗日乘子，而且不等式约束\(g_i(w)\leq0\)，要求其拉
格朗日乘子\(\alpha_i\geq0\)。此时只要不满足等式或者不等式约束，也就是存在一个或者多个\(i\)使得\(g_i(w) \ge 0\)或者
\(h_i(w)\neq0\)，那么求解\(\theta_p(w)\)的结果必然是无穷大；而当所有的\(i\)都满足约束时，\(\theta_p(w)=f(w)\)，此时两者
等价，再对\(\theta_p(w)\)参数\(w\)求最小，即为原始问题。也就是说，两者是等价的，相当于同一个问题。

\begin{align}
\theta_p(w) = \left\{ \begin{array}{} f(w) & 如果w满足原约束 \\
\infty & 否则 \end{array} \right.
\end{align}

而对偶问题就是调节一下求解的参数的顺序。原问题中先对参数\(\alpha,\beta\)求最大，然后对\(w\)求最小；对偶问题中先对参数
\(w\)求最小，然后再对参数\(\alpha,\beta\)求最大，也就是对调了 max 和 min 的求解顺序，仅此而已。

由于\(max min(\cdots) \leq min max(\cdots)\)，所以\(d^* \leq p^*\)。并且当\(w,\alpha,\beta\)满足 KKT 条件时，原问题的解和
对偶问题的解相同即\(p^*=d^*\)。反之也成立，即如果原问题和对偶问题的解相同，那么\(w,\alpha,\beta\)满足 KKT 条件。

\begin{align}
\frac{\partial}{\partial w_i} \mathcal{L}(w^*,\alpha^*,\beta^*) & = 0, \quad i=1,2,\ldots,n \\
\alpha_i^*g_i(w^*) & = 0, \quad i=1,2,\ldots,k \label{kkt:ducom} \\
g_i(w^*) & \leq 0, \quad i=1,2,\ldots,k \\
\alpha^* & \geq 0, \quad i=1,2,\ldots,k \\
h_i(w) & = 0, \quad i=1,2,\ldots,l \\
\end{align}

公式\(\eqref{kkt:ducom}\)称为 KKT 对偶互补条件（KKT dual complementary condition）；由此条件可知，若\(\alpha_i^* > 0\)，
则\(g_i(w^*) = 0\)。这个条件是说明 SVM 只有少数支撑向量的关键，同时也用于证明 SMO 算法收敛性。

具体到 SVM 算法，原问题的拉格朗日函数是

\begin{align}
\mathcal{L}(w,b,\xi,\alpha,\mu) = & \frac{1}{2}||w||^2 + C\sum_{i=1}^m\xi_i \notag \\
& - \sum_{i=1}^m\alpha_i[y^{(i)}(w \cdot x^{(i)} + b)-1 + \xi_i] - \sum_{i=1}^m\mu_i\xi_i \\
其中 & \alpha_i \geq 0; \mu_i \geq 0 \quad 两者都是拉格朗日乘子 \notag
\end{align}

对偶问题是拉格朗日函数的极大极小问题。首先求\(\mathcal{L}(w,b,\xi,\alpha,\mu)\)对\(w,b,\xi\)求极小，分别求导并令导数为0

\begin{align}
\nabla_w \mathcal{L}(w,b,\xi,\alpha,\mu) & = w - \sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} = 0 \\
\nabla_b \mathcal{L}(w,b,\xi,\alpha,\mu) & = -\sum_{i=1}^m \alpha_i y^{(i)} = 0 \\
\nabla_{\xi_i} \mathcal{L}(w,b,\xi,\alpha,\mu) & = C - \alpha_i -\mu_i = 0
\end{align}

得到

\begin{align}
w=\sum_{i=1}^m \alpha_i y^{(i)} x^{(i)} \\
\sum_{i=1}^m \alpha_i y^{(i)} = 0 \\
C- \alpha_i -\mu_i = 0
\end{align}

将结果带入原问题的拉格朗日函数，

\begin{align}
min_{w,b,\xi} \mathcal{L}(w,b,\xi,\alpha,\mu) = -\frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m \alpha_i\alpha_j 
y^{(i)}y^{(j)} \langle x^{(i)}x^{(j)} \rangle + \sum_{i=1}^m\alpha_i
\end{align}

再对\(min_{w,b,\xi}\mathcal{L}(w,b,\xi,\alpha,\mu)\)求参数\(\alpha\)的极大，就得到了对偶问题目标函数的表达式，连同上面得
到的约束，共同构成对偶问题：

\begin{align}
max_\alpha \quad & = -\frac{1}{2}\sum_{i=1}^m\sum_{i=1}^m \alpha_i\alpha_j y^{(i)}y^{(j)} 
\langle x^{(i)}x^{(j)}\rangle + \sum_{i=1}^m\alpha_i \\
s.t. \quad & = \sum_{i=1}^m \alpha_i y^{(i)} = 0 \\
& C - \alpha_i - \mu_i = 0, \quad i=1,2,\ldots,m \\
& \alpha_i = 0, \quad i=1,2,\ldots,m \\
& \mu_i \geq = 0, \quad i=1,2,\ldots,m
\end{align}

最后利用倒数第三个等式约束消去变量\(\mu_i\)，只留下变量\(\alpha_i\)，得到\(0 \leq \alpha_i \leq C\)，同时将目标函数中的
输入属性的內积\(\langle x^{(i)},x^{(j)} \rangle\)替换成核函数\(\langle \phi(x^{(i)}),\phi(x^{(j)}) \rangle\)，并且直接使
用核函数的最终形式\(K(x^{(i)},x^{(j)})\)得到对偶问题的最终形式

\begin{align}
max_{\alpha} \quad & W(\alpha) = \sum_{i=1}^m \alpha_i - 
\frac{1}{2} \sum_{i,j=1}^m y^{(i)}y^{(j)} \alpha_i\alpha_j K( x^{(i)},x^{(j)} ) \\
s.t. \quad & 0 \leq \alpha_i \leq C, \quad i = 1,2,\ldots,m \\
& \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align}

***** Kernel
使用核函数的方法：将原始输入的属性值\(x\)变换成\(\phi(x)\)特征作为算法的输入（仅此而已，不知道为什么原来就一直没有看懂）。
只是在具体运用时利用了一点小技巧，并不是直接去计算映射后的值然后再去计算，而是先将原始输入属性值表示称內积的形式，然后巧
妙的用核函数来代替內积。这样做的优势：将核函数代替內积可以高效计算；同时可以将特征映射到高维空间，从而将原来线性不可分的
问题转换成线性可分。

一般来说，如果输入空间\(x^{(i)} \in \mathbb{R}^n\)，对应的标记有两类\(y^{(i)} \in \{-1,1\}\)，如果能用\(\mathbb{R}^n\)中
的一个超曲面将正负实例正确分开，则称这个问题为非线性可分问题。而非线性问题往往不好求解，一般采取非线性变换， *将非线性问
题转换成线性问题* ，通过求解变换后的线性问题来得到原来的非线性问题的解。

用线性分类方法求解非线性问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练
数据中学习分类模型。核技巧就是这样的方法。支撑向量机使用核技巧的基本想法就是通过一个非线性变换将输入空间对应到一个特征空
间，使得在输入空间\(\mathbb{R}^n\)中的超曲面对应于特征空间\(\mathcal{H}\)中的超平面，这样学习任务通过在特征空间中求解线
性支持向量机就可以完成。其中输入空间为欧式空间\(\mathbb{R}^n\)或离散空间，特征空间为希尔伯特空间\(\mathcal{H}\)（no see）。

设\(\mathcal{X}\)是输入空间，\(\mathcal{H}\)为特征空间，如果存在一个从\(\mathcal{X}\)到\(\mathcal{H}\)的映射，
\[\phi(x):\mathcal{X} \to \mathcal{H} \]使得对所有的\(x,z \in \mathcal{X}\)，函数\(K(x,z)\)满足\[K(x,z)=\phi(x) \cdot
\phi(z)\]则称\(K(x,z)\)为核函数。

核函数的想法是，在学习和预测时，只使用核函数\(K(x,z)\)，而不显示的定义映射函数\(\phi\)，这将比直接计算\(\phi(x) \cdot
\phi(z)\)容易的多。由于算法中所有的属性值（例如目标函数和决策函数）都可以表示成內积的形式\(\langle x,z \rangle\)，因为需
要将所有的\(x\)都替换成\(\phi(x)\)，那么直接将內积替换成\(\langle\phi(x),\phi(z)\rangle\)的形式，而
\(\langle\phi(x),\phi(z)\rangle\)就是一个核函数，直接带入\(K(x,z)\)的表达式就可以。最终结果就是将算法中所有的內积都直接
替换成核函数即可；根本无需计算映射，也根本无需知道映射函数的表达式，只需要使用核函数的最终表达式。而且核函数并不单单可以
应用在支撑向量机上，所有可以将输入属性表示成內积的形式的算法都可以使用。

对于给定的核\(K(x,z)\)，特征空间\(\mathcal{H}\)和映射函数\(\phi\)的取法并不唯一。特征空间可以不同，即便在同一个特征空间
也可以取不同的映射。

TODO 举一个核函数和映射函数表达式的例子

*核函数的选取：* 有时可以选择标准的核函数，有时需要自己根据问题构造核函数（需要阅读相应的论文来了解怎样为一个新问题发明
一个新的核函数）。

通常所说的核函数就是正定核函数（positive definite kernel function）。根据 Mercer 定理，正定核函数的 *充要条件* ：设已知
\(K:\mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R} \)，则\(K(x,z)\)是正定核函数的充要条件是对任意
\(\{x^{(1)},x^{(2)},\ldots,x^{(m)}\},\ (m < \infty) \)，相应的核矩阵 Gram 矩阵\(K=[ K(x^{(i)},x^{(j)}) ]_{m \times m} \)
是对称半正定的。

*常用核函数:* 高斯核函数（Gaussian kernel function）\[ K(x,z) = exp \left( -\frac{||x-z||^2}{2\sigma^2} \right) \] 多项
式核函数（polynomial kernel function）\[ K(x,z) = (x^T z +c)^d \] 字符串核函数（string kernel function）

支撑向量机，通过核函数将数据映射到高维空间只是增大了数据线性可分的可能性，但无法确保映射后一定线性可分。因此需要使用
\(\ell 1\)正则化来修正模型；同时也使得分割线对 outliers 不那么敏感。

***** 决策函数
决策函数即算法最终得到的分离超平面的表达式。分离超平面\(w^*,b^*\)的表达式由原问题通过满足 KKT 条件求解得到，而表达式中参
数具体的值由对偶问题通过 SMO 算法求得。原问题可以表示为

\begin{align}
min_{w, b,\xi_i} \quad & \frac{1}{2}||w||^2 + C\sum_{i=1}^m \xi_i \\
s.t. \quad & -[y^{(i)} \left( w \cdot x^{(i)} + b \right) - 1 + \xi_i] \leq 0, \quad i=1,2,\ldots,m \\
& -\xi_i \leq 0, \quad i=1,2,\ldots,m
\end{align}

拉格朗日函数

\begin{align}
\mathcal{L}(w,b,\xi,\alpha,\mu) = & \frac{1}{2}||w||^2 + C\sum_{i=1}^m\xi_i \notag \\
& - \sum_{i=1}^m\alpha_i[y^{(i)}(w \cdot x^{(i)} + b)-1 + \xi_i] - \sum_{i=1}^m\mu_i\xi_i 
\end{align}

解满足 KKT 条件

\begin{align}
& \partial_w\mathcal{L}(w^*,b^*,\xi^*,\alpha^*,\mu^*) = w^* - \sum_{i=1}^m \alpha_i^* y^{(i)} x^{(i)} = 0 \\
& \partial_b\mathcal{L}(w^*,b^*,\xi^*,\alpha^*,\mu^*) = -\sum_{i=1}{m} \alpha_i^* y^{(i)} = 0 \\
& \partial_{\xi}\mathcal{L}(w^*,b^*,\xi^*,\alpha^*,\mu^*) = C - \alpha^* - \mu^* = 0 \\
& \alpha_i^* [y^{(i)} \left( w \cdot x^{(i)} + b \right) - 1 + \xi_i] = 0 \\
& \mu_i^* \xi_i^* = 0 \\
& -[y^{(i)} \left( w \cdot x^{(i)} + b \right) - 1 + \xi_i] \leq 0 \\
& -\xi_i^* \leq 0 \\
& \alpha_i^* \geq 0 \\
& \mu_i^* \geq 0
\end{align}

求解上面的方程，\(w^*\)较易求解。再由 KKT 对偶互补条件可知，当存在\(\alpha_i^*\)满足\(0 < \alpha_i^* < C\)时，
\(y^{(i)}(w^* \cdot x^{(i)} + b^*) - 1 = 0\)，从而可求得\(b^*\)。其中会利用一个小技巧\(y^{(i)} \cdot y^{(i)} = 1\)，并用
核函数替换內积最终得到

\begin{align}
w^* & = \sum_{i=1}^m \alpha_i^* y^{(i)} x^{(i)} \\
b^* & = y^{(i)} - \sum_{i=1}^m \alpha_i^* y^{(i)} K(x^{(i)}, x^{(j)}) \\
& \sum_{i=1}^m \alpha_i^* y^{(i)} K(x \cdot x^{(i)} ) + b^* = 0 \\
f(x) & = sign \left( \sum_{i=1}^m \alpha_i^* y^{(i)} K(x \cdot x^{(i)}) + b^* \right)
\end{align}

***** 支撑向量
在线性不可分的情况下，将对偶问题的解中对应\(\alpha_i^* > 0\)的样本点\((x^{(i)},y^{(i)})\)称为支撑向量。软间隔的支撑向量
可能在任何地方：可以在间隔边界上；可以在间隔边界与分离超平面之间；也可以在分离超平面误分的一侧。
+ 若\(\alpha_i^* < C\)，则\(\xi_i=0\)：支撑向量落在边界线上
+ 若\(\alpha_i^* = C, \quad 0 < \xi_i < 1\)，则分类正确：支撑向量在间隔边界与分离超平面之间
+ 若\(\alpha_i^* = C, \quad \xi_i = 1\)，则支撑向量位于分离超平面上
+ 若\(\alpha_i^* = C, \quad \xi_i > 1\)，则支撑向量位于分离超平面误分的一侧
note：\(0 \leq \alpha_i^* \leq C, \quad 1-\xi_i\)是函数间隔；只有\(\alpha_i \ne 0\)对应的样本点才是支撑向量？？？可能
KKT 对偶互补条件中两个变量都为零？？？ TODO


***** Sequential Minimal Optimization
*坐标上升法：* 当求解多变量最优化问题且不存在约束的时候，可以使用坐标上升法（和梯度下降法以及牛顿法都是最优化方法）来求
解。利用两层循环来实现，外层循环便利所有样本，内层循环便利所有变量。在内层循环中每次仅优化一个变量，同时固定其他的变量不
变，针对该变量来优化目标函数。这样总是沿着和坐标轴平行的方向取得最大值，而且选取往最优解移动的速度最快的变量来求解。

序列最小最优化算法求解的对象是凸二次规划的对偶问题：

\begin{align}
max_{\alpha} \quad & W(\alpha) = \sum_{i=1}^m \alpha_i - 
\frac{1}{2} \sum_{i,j=1}^m y^{(i)}y^{(j)} \alpha_i\alpha_j K(x^{(i)},x^{(j)}) \\
s.t. \quad & 0 \leq \alpha_i \leq C, \quad i = 1,2,\ldots,m \\
& \sum_{i=1}^m \alpha_i y^{(i)} = 0
\end{align}

在该问题中变量是拉格朗日乘子\(\alpha_i\)，每个样本都有一个拉格朗日乘子，变量的总数等于训练样本的个数\(m\)

算法基本思路：因为 KKT 条件是该最优化问题的充分必要条件，当所有变量都满足该最优化问题的 KKT 条件，那么这个最优化问题的解
就得到了。否则，选择两个变量，固定其他变量，针对这两个变量构建二次规划问题，而求解该二次规划子问题将使得目标函数值变大。
这样将问题不断分解为子问题，并对子问题求解，直到所有的变量都满足 KKT 条件。

由于构建的二次规划子问题可以通过解析的方法求解，这样就大大提高了整个算法的计算速度（迭代会很耗时的）。另外由于约束
\(\sum_{i=1}^m \alpha_i y^{(i)} = 0\)的存在，无法只更改一个变量，因为当其他的变量值都不改变时，该变量的值也会由于约束的存在
而固定无法改变。所以子问题每次都会同时更新两个变量，在满足约束的条件下来求解二次规划问题。

SMO 是启发式算法：好像两个变量的选择方法是启发式搜索算法（不确定）。

整个 SMO 算法包括两个部分：\(\textcircled{1}\)求解两个变量二次规划的解析方法；\(\textcircled{2}\)选择变量的启发式方法。

****** 两个变量二次规划的求解方法
每个子问题都可以转换成一个变量的二次函数，很容易求得解析解。

假设利用启发式方法选择出两个变量\(\alpha_1,\alpha_2\)，其他变量\(\alpha_3,\alpha_4,\ldots,\alpha_m\)固定不变，由约束可知
\( \alpha_1 y^{(1)} + \alpha_2 y^{(2)} = -\sum_{i=3}^{m} \alpha_i y^{(i)} \)由于公式右侧是固定的，使用一个常量符号
\(\zeta\)替代

\begin{equation}
\alpha_1 y^{(1)} + \alpha_2 y^{(2)} = \zeta
\end{equation}

虽然要同时更新两个变量，但其实只有一个自由变量。此处使用\(\alpha_2\)表示\(\alpha_1\)，利用\({(y^{(1)})}^2 = 1\)

\begin{equation}
\alpha_1 = ( \zeta - \alpha_2 y^{(2)} ) y^{(1)}
\end{equation}

因此目标函数可以写成

\begin{equation}
W(\alpha_1,\alpha_2,\ldots,\alpha_m) = W((\zeta - \alpha_2 y^{(2)})y^{(1)},\alpha_2,\alpha_3,\ldots,\alpha_m)
\end{equation}

由于将\(\alpha_3,\alpha_4,\ldots,\alpha_m\)视为固定值，目标函数可以看做关于\(alpha_2\)的二次函数，可以写成
\(a\alpha_2^2 + b\alpha_2 + c\)的形式，在没有约束的情况下，可以很容易的通过求导并令导数为零得到极值。

同时每个变量都必须满足约束\(0 < \alpha_i < C\)，具体到一个子问题上，对于变量\(\alpha_1,\alpha_2\)，两个变量都必须约束在
\([0,C] \times [0, C]\)的方框中，再加上上面的线性约束，\(\alpha_1,\alpha_2\)必须约束在被方框截断的直线上。从而\(L \leq
\alpha_2^{new} \leq H\)，其中L与H是\(\alpha_2^{new}\)所在的对角线段端点的界。另外线性约束中的常量\(\zeta\)可以用
\(\alpha_1 \pm \alpha_2\)表示

\begin{align}
& if \ y^{(1)} \neq y^{(2)} \notag \\
& L=max(0, \alpha_2^{old} - \alpha_1^{old}),  H=min(C,C+\alpha_2^{old}-\alpha_1^{old}) \\
& if \ y^{(1)} = y^{(2)} \notag \\
& L=max(0,\alpha_2^{old} + \alpha_1^{old}-C), H=min(C,\alpha_2^{old}+\alpha_2{old}) 
\end{align}

先只要求满足线性约束，求解得到\alpha_2^{new,unclipped} 然后再裁剪来满足 box constraints

\begin{align}
\alpha_2^{new} = \left\{ \begin{array}{} H & if \alpha_2^{new,unclipped} > H \\
\alpha_2^{new,unclipped} & if L \leq \alpha_2^{new,unclipped} \leq H \\
L & if \alpha_2^{new,unclipped} < L \end{array} \right.
\end{align}

再利用线性约束求得\(\alpha_1^{new}\)

\begin{equation}
\alpha_1^{new} = \alpha_1^{old} + y^{(1)}y^{(2)} (\alpha_2^{old} - \alpha_2^{new})
\end{equation}

\(\alpha_2\)的求解：为了叙述方便，记\[g(x)=\sum_{i=1}^m \alpha_i y^{(i)} K(x^{(i)},x) + b\] 令
\[E_i = g(x^{(i)}) - y^{(i)} = (\sum_{j=1}^m \alpha_j y^{(j)} K(x^{(j)},x^{(i)}) + b) - y^{(i)}, \quad i=1,2 \]
用于表示g(x)对输入x^{(i)}的预测值与真实值y^{(i)}之差。最终可得

\begin{align}
\alpha_2^{new,unclipped} = \alpha_2^{old} + \frac{y^{(2)} (E_1 - E_2)}{\eta} \\
\eta = K_{11} + K_{22} -2K_{12} = ||\phi(x^{(1)}) - \phi(x^{(2)}||^2
\end{align}

其中\(\phi(x)\)是输入空间到特征空间的映射

****** 变量选择方法
SMO 称第一个变量的选择为外层循环，第二个变量的选择为内层循环。外层循环在训练样本中选择违反 KKT 条件最严重的样本点，将其
对应的变量作为第一个变量。内层循环的标准是希望选择的变量有足够大的变化。

\begin{align}
KKT 条件 \notag \\
\alpha_i = 0 \Leftrightarrow y^{(i)}g(x^{(i)}) \geq 1 \\
0 < \alpha_i < C \Leftrightarrow y^{(i)}g(x^{(i)})=1 \\
\alpha_i = C \Leftrightarrow y^{(i)}g(x^{(i)}) \leq 1 \\
其中 g(x^{(i)}) = \sum_{j=1}^m \alpha_j y^{(j)} K(x^{(i)}, x^{(j)}) + b
\end{align}

外层循环首先遍历所有满足条件\(0 < \alpha_i < C\)的样本点，即在间隔边界上的支撑向量点，检验他们是否满足 KKT 条件；如果这
些样本点都满足 KKT 条件，那么遍历整个训练集，检验他们是否满足 KKT 条件。这里的满足 KKT 条件都有一定的误差容忍范围，典型
值为0.001~0.01。什么叫做违反最严重？实际值与要求值差别比较大？ TODO

内层循环选择\(\alpha_2\)，由于\(\alpha_2^{new}\)依赖于\(|E_1 - E_2|\)，一种简单的做法是通过使\(|E_1 - E_2|\)最大来使得
\(\alpha_2\)有足够大的变化。由于\(\alpha_1\)已经确定，\(E_1\)也随之确定。如果\(E_1\)是正的，那么选择最小的\(E_i\)作为
\(E_2\)；如果\(E_1\)是负的，那么选择最大的\(E_i\)作为\(E_2\)。

为了节省计算时间，将所有的\(E_i\)值保存在一个列表中。

在特殊情况下，如果内层循环通过以上方法选择的\(\alpha_2\)不能使目标函数有足够的上升，那么采用以下启发式规则继续选择
\(\alpha_2\)。遍历在间隔边界上的支撑向量点，依次将其对应的变量作为\(\alpha_2\)试用，直到目标函数有足够的上升。若找不到合
适的\(\alpha_2\)，那么遍历训练数据集；若仍找不到合适的\(\alpha_2\)，则放弃之前选择的\(\alpha_1\)，再通过外层循环寻找另外
的\(\alpha_1\)。注：这里目标函数是上升还是下降要看目标函数具体是在求最大还是最小。

计算阈值\(b\)和差值\(E_i\)：在每次完成两个变量的优化后，都要重新计算阈值\(b\)。当\(0 < \alpha_1^{new} < C\)时，由 KKT 条
件可知\(\sum_{i=1}^m \alpha_i y^{(i)} K_{i1} + b = y^{(1)}\)和\(E_1\)的定义

\(E_1 = \sum_{i=3}^m \alpha_i y^{(i)} K_{i1} + \alpha_1^{old}y^{(1)}K_{11} + \alpha_2^{old}y^{(2)}K_{21} + b^{old} - y^{(1)}\)可知

\begin{align}
b_1^{new} & = y^{(1)} - \sum_{i=3}^m \alpha_i y^{(i)} K_{i1} - \alpha_1^{new}y^{(1)}K_{11} - \alpha_2^{new}y^{(2)}K_{21}
\\
& = -E_1 - y^{(1)}K_{11}(\alpha_1^{new} - \alpha_1^{old}) - y^{(2)}K_{21}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}
\end{align}

同样，如果\(0 < \alpha_2^{new} < C)，那么

\begin{align}
b_2^{new} = -E_2 - y^{(1)}K_{12}(\alpha_1^{new} - \alpha_1^{old}) - y^{(2)}K_{22}(\alpha_2^{new} - \alpha_2^{old}) + b^{old}
\end{align}

如果\(\alpha_1^{new},\alpha_2^{new}\)同时满足\(0 < \alpha_i^{new} < C, i=1,2\)，那么\(b_1^{new}=b_2^{new}\)；如果
\(\alpha_1^{new},\alpha_2^{new} = 0 \ or \ C\)，那么\(b_1^{new},b_2^{new}\)以及他们之间的数都满足 KKT 条件，这时选择
\(b_1^{new},b_2^{new}\)的中点作为\(b^{new}\)。

另外再每次更新完两个变量后，还必须更新对应的\(E_i\)值，并将他们保存到列表中。

\begin{align}
E_i^{new} = \sum_S y^{(j)} \alpha_j K(x^{(i)},x^{(j)}) + b^{new} - y^{(i)}
\end{align}

其中S是所有支撑向量\(x^{(j)}\)的集合。

** Unsupervised Learning
无监督学习

** Semi-supervised Learning
半监督学习

** Reinforcement Learning
强化学习

** Computational Learning Theory
计算学习理论用于真正理解机器学习算法，了解怎样修改算法；区分开真正了解算法与只看了书和公式的人，真正成为一个好的木匠。

思考：
+ 学习器(机器的或非机器的)应遵循什么样的规则？
+ 是否可能独立于学习算法确定学习问题中固有的难度？
+ 能否知道为保证成功的学习有多少训练是必要的或充足的？
+ 能否刻画出一类学习问题中固有的计算复杂度？
对所有这些问题的一般回答还未知。这里着重讨论只给定目标函数的训练样例和候选假设空间的条件下,对该未知的目标函数的归纳学习
问题。在这样的框架下,主要要解决的问题如:需要多少训练样例才足以成功地学习到目标函数,以及学习器在达到目标前会有多少次出错。
并对这些问题提出定量的上下界。


该理论致力于回答如下的问题:
+ 在什么样的条件下成功的学习是可能的？
+ 在什么条件下一特定的学习算法可保证成功运行？
+ 我们真正关心的是泛化误差，却为什么始终在努力减小经验误差？
+ bias / variance 的准确定义

learning theory split to two central questions:
1. can we make sure that Eout(g) is close enough to Ein(g) ?
2. can we make Ein(g) small enough ?

为了解决这些问题，需要许多特殊的条件假设：
+ 怎样定义学习得到的结果是成功的？是必须找到目标概念？还是以较大的概率得到目标概念的近似？
+ 学习器如何得到训练样本？学习器自己由实验获取？还是按照某过程随机的生成而不受学习器的控制？

训练样本集 D 中所有从隐含未知分布 \(\mathcal{D}\) 上独立采样得到，h 是一个从输入空间 \(\mathcal{X}\) 到标记空间
\(\mathcal{Y}\) 的一个映射，h 在 D 上的经验误差（也就是训练误差）为\[ \hat{R}(h;D) = \frac{1}{m} \sum_{i=1}^{m}
\mathit{1}(h(x^{(i)} \ne y^{(i)}) \]泛化误差为\[ R(h;\mathcal{D}) = P_{x \sim \mathcal{D}} (h(x) \ne y) \]由于 D 是
\(\mathcal{D}\) 的独立同分布采样，因此 *h 的经验误差的期望等于其泛化误差。* 即\[ E[ \hat{R}(h;D) ] = R(h; \mathcal{D})
\]

note：均值是观察样本的平均值，尽管随机变量一样，但观察到的样本不同，均值很可能不同；期望是一个数学特征，针对于一个随机变
量。根据大数定律（随机变量序列的前一些项的算术平均值，在某种条件下，收敛到这些项的均值的算术平均值），期望是均值随着样本
趋于无穷时的极限。

频率的稳定性是概率定义的客观基础。

中心极限定理：在某种一条件下，大量随机变量之和的分布逼近于正态分布。

*** Probably Approximately Correct
概率近似正确，简称 PAC：通常我们无法精确的学到目标概念（concept）。原因如下：
+ 由于训练集 D 往往只包含有限数量的样本，因此，通常会存在一些在 D 上的“等效”的假设，学习算法无法区分。即存在多个假设都满
  足样本空间到标记空间的映射
+ 从分布\(\mathcal{D}\)上采样得到 D 的过程有一定的偶然性。存在一定的概率使得样本都有某种特性，但该特性在总体中不存在；从
  而对同样大小的不同训练集，学的的结果也可能有所不同
因此我们希望以比较大的把握学的比较好的模型；即以 *较大的概率* 学的 *误差满足预设上限* 的模型。也就成为了“概率”“近似正确”
框架。

假设输入空间\(\mathcal{X}\)中所有样本服从一个 /隐含未知/ 的分布\(\mathcal{D}\)，训练集 D 中所有样本都是独立地从这个分布
上采样而得（独立同分布：independent and identically distribution， 简称 i.i.d)。通常假设训练集和测试集服从相同的分布；训
练集的样本独立同分布。

PAC 可学习(PAC Learnable)：只要从分布\(\mathcal{D}\)中独立同分布采样得到的样例数目 m ，满足\(m \geq
poly(\frac{1}{\epsilon},\frac{1}{\delta},size(x),size(c))\)，学习算法能从假设空间\(\mathcal{H}\)中 PAC 辨识概念类
\(\mathcal{C}\)，则称概念类\(\mathcal{C}\)对假设空间\(\mathcal{H}\)而言是 PAC 可学习的，有时也简称概念类\(\mathcal{C}\)
是 PAC 可学习的。若算法运行的时间也是多项式函数\(poly(\frac{1}{\epsilon},\frac{1}{\delta},size(x),size(c))\)，则称概念类
\(\mathcal{C}\)是高效 PAC 可学习的(efficiently PAC learnable)，称算法 L 是概念类\(\mathcal{C}\)的 PAC 学习算法。

*仅仅要求了训练样本的个数和时间满足一个多项式函数。*

假定学习算法处理每个样本的时间为常数，则算法的时间复杂度等价于样本复杂度。于是我们对算法时间复杂度的关心就转化成对样本复
杂度的关心。满足 PAC 学习算法 L 所需的\(m \geq poly(\frac{1}{\epsilon},\frac{1}{\delta},size(x),size(c))\)中最小的 m ，
称为学习算法 L 的样本复杂度。

不可知 PAC 可学习(agnostic PAC learnable)：

当 \(c \not \in \mathcal{H}\) 时，学习算法无法得到目标概念 c 的 \(\epsilon\) 近似。但是在假设空间 \(\mathcal{H}\) 中必存
在一个泛化误差最小的假设，找到此假设的 \(\epsilon\) 近似也不失为一个较好的目标。这是不可知学习。


PAC 学习给出了一个抽象地刻画机器学习能力的框架，基于这个框架能对很多重要的问题进行理论探讨：
+ 研究某任务在什么样的条件下可学得较好的模型？
+ 某算法在什么样的条件下可进行有效的学习？
+ 需多少训练样例才能获得较好的模型？

PAC 学习中一个关键因素是假设空间\(\mathcal{H}\)的复杂度；\(\mathcal{H}\)包含了学习算法 L 的所有可能输出，一般而言，
\(\mathcal{H}\)越大，其包含任意目标概念的可能性越大，但从中找到某个具体目标概念的难度也越大。\(|\mathcal{H}|\)有限时，我
们称\(\mathcal{H}\)为有限假设空间，否则称为无限假设空间。


*** Vapnik-Chervonenkis Dimension

Growth Function，增长函数：描述了假设空间 \(\mathcal{H}\) 的表示能力。具体定义为：增长函数 \(\prod_{\mathcal{H}} (m)\)
表示假设空间 \(\mathcal{H}\) 对 m 个示例所能赋予标记的最大可能结果数。\(\mathcal{H}\) 对示例所能赋予标记的可能结果数越大，
\(\mathcal{H}\) 的表示能力越强，对学习任务的适应能力越强；反应了假设空间的复杂度。

Dichotomy，对分：对二分类问题来说，\(\mathcal{H}\) 中的假设对 D 中示例赋予标记的每种可能结果称为对 D 的一种对分；每个假
设会把示例集分为两类，因此称为对分。

Shattering，打散：若假设空间 \(\mathcal{H}\) 能实现对示例集 D 的所有对分，即 \(\prod_{\mathcal{H}} = 2^m\) ，则称示例集
D 能被假设空间 \(\mathcal{H}\) 打散。


现实学习任务所面临的通常是无限假设空间。假设空间无限和 VC 维有限的关系？？


*假设空间 \(\mathcal{H}\) 的 VC 维是能被 \(\mathcal{H}\) 打散的最大示例集的大小，* 即

\begin{equation}
VC(\mathcal{H}) = max\{ m: \prod_{\mathcal{H}} (m) = 2^m \}
\end{equation}


增长函数的上界（增长函数与 VC 维的关系）
若假设空间 \(\mathcal{H}\) 的 VC 维为 d，

\begin{align}
& \prod_{\mathcal{H}} \leq \sum_{i=0}^{d} \left( \array {m \\ i} \right), \quad m \in \mathbb{N}, Sauer 引理 \\
& \prod_{\mathcal{H}} \leq \left( \frac{e \cdot m}{d} \right) ^d, \quad m > d, 推论
\end{align}

增长函数可用于估计经验误差与泛化误差之间的关系：对假设空间 \(\mathcal{H}, m \in \mathbb{N}, 0 < \epsilon < 1, 任意 \in
\mathcal{H}\) 有

\begin{equation}
P \left( \left| E(h) - \hat{E}(h) \right| > \epsilon \right) \leq 4\prod_{\mathcal{H}} (2m) exp(- \frac{m\epsilon^2}{8})
\end{equation}

泛化误差界：只与样本个数 m 有关；基于 VC 维的泛化误差界是分布无关(distribution-free)、数据独立(data-independent)的。
*定理：* 若假设空间 \(\mathcal{H}\) 的 VC 维为 d，则对任意 \(m > d, \ 0 < \delta < 1, h \in \mathcal{H}\) 有

\begin{equation}
P \left( \left| E(h) - \hat{E}(h) \right| \leq \sqrt{\frac{8dln\frac{2em}{d} + 8ln\frac{4}{\delta}}{m}} \right) \geq 1 -
\delta
\end{equation}

*定理：任何 VC 维有限的假设空间 \(\mathcal{H}\) 都是（不可知） PAC 可学习的*

*** 不等式定理

The union bound ：\(A_1,A_2,\ldots,A_k\)是 k 个不同的事件，可能并不独立，那么

\begin{equation}
P(A_1 \cup A_2 \cup \ldots \cup A_k) \leq P(A_1) + P(A_2) + \ldots + P(A_k)
\end{equation}

Jensen inequality：对任意的凸函数 f(x) ，有

\begin{equation}
f(E[x]) \leq E[f(x)]
\end{equation}

Hoeffding inequality，霍夫丁不等式：若\(x_1,x_2,\ldots,x_m\)为 m 个独立随机变量，且满足 \(0 \leq x_i \leq 1\)，则对任意
\(\epsilon > 0\)，有 

\begin{align}
P \left( \frac{1}{m} \sum_{i=1}^{m} x_i - \frac{1}{m} \sum_{i=1}^{m} E(x_i) \geq \epsilon \right) \leq exp(-2m \epsilon^2) \\
P \left( \left| \frac{1}{m} \sum_{i=1}^{m} x_i - \frac{1}{m} \sum_{i=1}^{m} E(x_i) \right| \geq \epsilon \right) \leq
2exp(-2m \epsilon^2)
\end{align}

McDiarmid inequality：若\(x_1,x_2,\ldots,x_m\)为 m 个独立随机变量，且对任意 \(1 \leq i \leq m\)，函数 f 满足
\[ sup_{x_1,\ldots,x_m,x_i^,} | f(x_1,\ldots,x_m) - f(x_1,\ldots,x_{i-1},x_i^,,x_{i+1},\ldots,x_m) | \leq c_i \]
则对任意 \(\epsilon > 0\)，有

\begin{align}
P(f(x_1,\ldots,x_m) - E[f(x_1,\ldots,x_m)] \geq \epsilon ) \leq exp(\frac{-2\epsilon^2}{\sum_i c_i^2}) \\
P( | f(x_1,\ldots,x_m) - E[f(x_1,\ldots,x_m)] | \geq \epsilon ) \leq 2exp(\frac{-2\epsilon^2}{\sum_i c_i^2}) 
\end{align}

*** 术语
*concept* 概念：表示从样本空间\(\mathcal{X}\)到标记空间\(\mathcal{Y}\)的映射，用符号 c 表示。不要疑惑为什么叫做概念，在机
器学习中，这个映射就被称为概念。

*目标概念* ：若对任何样例\((x,y)\)都有\(c(x)=y\)成立，则称 c 为目标概念。

*concept class* 概念类：我们希望学的的目标概念构成的集合称为概念类，用符号\(\mathcal{C}\)表示。no see 概念类到底是个什么
东西？？？？

*hypothesis space* 假设空间：对给定的学习算法 L ，她所考虑的所有可能概念的集合称为假设空间，用符号\(\mathcal{H}\)表示。
学习算法吧自认为可能的目标概念一起构成\(\mathcal{H}\)，学习算法并不可能知道概念类的真实值，因此\(\mathcal{C}\)和
\(\mathcal{H}\)通常是不同的。假设空间中任何一个假设\(h \in \mathcal{H}\)也都是从样本空间\(\mathcal{X}\)到标记空间
\(\mathcal{Y}\)的映射。

*consistent* 一致的：若目标概念 \(c \in \mathcal{H}\) ，则表示\(\mathcal{H}\)中存在假设能将所有示例按照与真实标记一致的
方式分开，则称该问题对学习算法 L 是 /可分的/ (separable)，也称为一致的；若 \(c \not \in \mathcal{H}\)，则称为不可分
(non-separable)或者不一致(non-consistent)

*PAC Identity* PAC 辨识：对\(0 < \epsilon, \delta < 1\)，所有\(c \in \mathcal{C}\)和所有的分布\(\mathcal{D}\)，若存在学
习算法 L ，其输出假设\(h \in \mathcal{H}\)满足\[ P(R(h) \leq \epsilon ) \geq 1 - \delta \]则称学习算法能从假设空间
\(\mathcal{H}\)中 PAC 辨识概念类\(\mathcal{C}\)


* 附

** AI-ML-DL
人工智能就是想让计算机拥有自行处理高级任务的能力；机器学习是实现人工智能的一种方法；深度学习是一类具体的机器学习方法。
*** Artificial Intelligence
人工智能发展：
+ 推理期 -- 赋予机器逻辑推理能力，但没有知识
+ 知识期 -- 让机器拥有知识（专家系统）；由人来把知识总结出来再教给计算机
+ 机器学习 -- 让机器自己能够学习知识

*** Machine Learning
机器学习研究方法：
+ 连接主义（connectionism）学习 -- 代表：感知机、神经网络、深度学习
+ 符号主义（symbolism）学习 -- 代表：决策树
+ 统计学习 -- 代表：SVM、核方法；（跟上述两种方法并不是完全并列）

*** Deep Learning
深度学习
**** 现在火热的原因
+ 数据量大了
+ 计算能力强了

**** 降低门槛
以往机器学习技术在应用中要取得良好的性能，对使用者的要求较高；而深度学习只要超参调节的好，性能往往就很好，显著降低了机器
学习应用者的门槛。

手工调参：参数的设置缺乏理论指导，参数调节上失之毫厘，学习结果谬以千里；使用需要大量的 track （窍门）

**** 理论基础
深度学习缺乏严格的理论基础

** 归纳偏好
*** No Free Lunch Theorem -- NFL
没有免费的午餐定理：所有问题出现的机会相同，或所有问题同等重要的前提下，无论算法 A 看似多精妙，算法 B 多笨拙，他们的期望
性能是相同的。即：若考虑所有潜在的问题，则所有学习算法都一样好。学习算法自身的归纳偏好与问题是否匹配，往往起到决定行的作
用。

脱离具体问题，空泛的谈什么学习算法更好毫无意义。

*** Occam's Razor
奥卡姆剃刀原理：在所有可能选择的模型中，能够很好的解释已知数据并且十分简单才是最好的模型。

* 心得






